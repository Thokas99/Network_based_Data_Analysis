---
title: "Exam_Lauria"
author: "Thomas Sirchi"
date: "`r Sys.Date()`"
output: html_document
---

## Loading packages

Store package names in a vectors for ease of access and to load them easily

```{r setup, message=FALSE, warning=FALSE}
# Define a vector of R packages
PACKAGES <- c(
  "GEOquery",        # For accessing and retrieving Gene Expression Omnibus (GEO) data
  "ggplot2",         # To create and customize plots, and save images
  "tidyverse",       # A collection of packages for easy and tidy data handling
  "factoextra",      # Provides additional functions for enhanced ggplot-based plots
  "FactoMineR",      # For multivariate exploratory data analysis
  "plotly",          # Interactive plotting library
  "dplyr",           # Efficient data manipulation and transformation
  "RColorBrewer",    # Color palettes for better visualizations
  "biomaRt",         # Interface to access BioMart databases
  "enrichR",         # For gene set enrichment analysis
  "xgboost",         # For the XGBoost (Extreme Gradient Boosting) method
  "randomForest",    # For random forest modeling
  "caret",           # For machine learning model training and evaluation
  "cluster",         # To perform PAM (Partitioning Around Medoids)
  "glmnet",          # To perform LASSO regression for features selection 
  "rScudo",          # To perform SCUDO (Signature-based Clustering for Diagnostic Purposes)
  "ggcorrplot",
  "corpcor",
  "HiClimR",
  "e1071",
  "corrplot",
  "ranger",
  "kknn",
  "LiblineaR"
  
)

invisible(lapply(PACKAGES, library, character.only = TRUE))
setwd("D:/VarieTHOM/University/QCB/4_SEMESTRE/Advanced Data Analysis")
gc()
```

Print current system info, R and packages versions (for reproducibility)

```{r sessionInfo}
sessionInfo()
```

## Data retrieval

Retrieve dataset from GEO

```{r Retrieve dataset from GEO, message=FALSE, warning=FALSE}
gset <- getGEO("GSE15235", GSEMatrix =TRUE)
```

Extract the data from the dataset

```{r Extract the data, message=FALSE, warning=FALSE}
exp_data <- data.frame(gset[["GSE15235_series_matrix.txt.gz"]]@assayData[["exprs"]])
exp_data <- na.omit(exp_data)

# Log trasform the data
mydata_log <- data.frame(log(exp_data))

```

# Metatdata manipulation

recover the gene ids and disease state from the metadata.

```{r gene.ids}

# general metatdata on the disesase
metadata <- data.frame(gset[["GSE15235_series_matrix.txt.gz"]]@phenoData@data)
disease_state <- subset(metadata, select = molecular.group.ch1)

# general metatdata on the Affimatrix, experiment, gene symbol etc. 
metadata2 <- data.frame(gset[["GSE15235_series_matrix.txt.gz"]]@featureData@data)

# Extract Gene Symbols from a column in the 'metadata2' data frame
Gene_Symbols <- data.frame(gsub("[ -/].*", "", metadata2$Gene.Symbol))

# Rename the column in the new data frame to "Gene.Symbol"
colnames(Gene_Symbols) <- c("Gene.Symbol")

# Set row names of 'Gene_Symbols' to match the row names of 'exp_data'
rownames(Gene_Symbols) <- rownames(exp_data)

# Check if there are any empty strings in the "Gene.Symbol" column
has_blanks <- any(is.na(Gene_Symbols$Gene.Symbol) | Gene_Symbols$Gene.Symbol == "")

# Print the result of whether there are any empty strings
head(has_blanks)

# Replace empty strings in the "Gene.Symbol" column with NA
Gene_Symbols <- Gene_Symbols %>%
  mutate(Gene.Symbol = na_if(Gene.Symbol, ""))

# Remove
rm(has_blanks,metadata , metadata2, gset)

```

## Fist view of the data

```{r Boxplots, message=FALSE, warning=FALSE}

# Create boxplot for original data
boxplot(head(exp_data,2000), main = "Original Data", col = "skyblue")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at 0

rm(exp_data)

# Create boxplot for normalized data
boxplot(head(mydata_log,2000), main = "Log Data", col = "pink")
abline(h = 0, lty = 2, col = "red")


```

# Unsupervised methods

```{r Principal component analysis PCA, message=FALSE, warning=FALSE}

# Transpose and add a response variable for future
t_data <- t(mydata_log)
t_data <- cbind(t_data, disease_state)

# Perform Principal Component Analysis (PCA) on the transpose of the cleaned data
res_pca <- prcomp(t(mydata_log))

# Extract eigenvalues and variance contributions from the PCA results
eig_PCA <- get_eig(res_pca)

# Transform the principal components into a data frame
components <- res_pca[["x"]]
components <- data.frame(components)

# Combine the principal components with the original disease_state metadata
components <- cbind(components, disease_state)

# Trigger garbage collection to free up memory
gc()

# Remove unnecessary objects
rm(eig_PCA)

```

# Display and save Scree plot

```{r Scree plot, message=FALSE, warning=FALSE}
# Visualize eigenvalues from PCA
fviz_eig(res_pca,addlabels = TRUE)
rm(res_pca)
```

# Plot PCA

```{r Plot PCA, message=FALSE, warning=FALSE}

# Create a 2D scatter plot using Plotly
fig2D <- plot_ly(components, 
                 x = ~PC1, y = ~PC2, 
                 color = components$molecular.group.ch1,
                 colors = brewer.pal(n = 4, name = "RdBu"),  
                 symbol = components$molecular.group.ch1, 
                 symbols = c('diamond', 'circle', 'cross'), 
                 mode = 'markers')

# Display the 2D scatter plot
fig2D

# Remove the 2D scatter plot object to free up memory
rm(fig2D)

```

# Unsupervised methods (machine learning methods)

## K-means

Compute k-means

```{r Compute k-means, message=FALSE, warning=FALSE}
set.seed(1234)

# Number of clusters
K <- 2                 

# Compute k-means
res_kmeans <- kmeans(t_data[,-ncol(t_data)], K)  

# Display a table of cluster assignments and disease states
table(res_kmeans$cluster, t(disease_state))

# Remove unnecessary objects
rm(K, res_kmeans)

```

# PAM (Partitioning Around Medoids)

```{r Find best K , message=FALSE, warning=FALSE}
set.seed(1234)

# Visualize the optimal number of clusters using the elbow method
fviz_nbclust(t(mydata_log), FUNcluster = cluster::pam, k.max = 4)

# Visualize the optimal number of clusters using the gap statistic
#fviz_nbclust(t(mydata_log), FUNcluster = cluster::pam, k.max = 4, method = 'gap_stat') + theme_classic()

# Visualize the optimal number of clusters using the within-cluster sum of squares method
fviz_nbclust(t(mydata_log), FUNcluster = cluster::pam, k.max = 4, method = "wss")

```

```{r Compute PAM , message=FALSE, warning=FALSE}
set.seed(1234)
K <- 3

# Perform PAM clustering
pam_result <- pam(t(mydata_log), K)

# Access cluster assignments and medoids
cluster_assignments <- pam_result$clustering
medoids <- pam_result$medoids

# Display a table of cluster assignments and disease states
table(cluster_assignments, t(disease_state))

# Plot the results (for 2D data)
plot(t(mydata_log), col = cluster_assignments, pch = 16, main = "PAM Clustering")
points(medoids, col = 1:K, pch = 3, cex = 2)

# Store cluster assignments in components$pam
components <- cbind(components, cluster_assignments)

# Create a plotly figure for visualization
fig_pam <- plot_ly(components, x = ~PC1, y = ~PC2, color = components$molecular.group.ch1,
                   colors = c("black", "red", "blue"), symbol = components$pam,
                   symbols = c('diamond', 'circle', 'cross'), mode = 'markers', type = "scatter")

fig_pam

# Remove unnecessary objects
rm(pam_result, fig_pam,K,cluster_assignments)


```

# hierarchical clustering

```{r hierarchical, message=FALSE, warning=FALSE}
# Calculate distances between observations and create a simple dendrogram
dm <- dist(t(mydata_log))
hc <- hclust(dm, method = 'average')
plot(hc, hang = -1)
rect.hclust(hc, k = 3, border = 'red')

# Assign cluster memberships
clust.vec.2 <- cutree(hc, k = 3)

# Visualize clusters
fviz_cluster(list(data = t(mydata_log), cluster = clust.vec.2))

# Remove unnecessary objects
rm(dm, hc, clust.vec.2)

```

# Supervised learning techniques

## LASSO (Least Absolute Shrinkage and Selection Operator)

```{r Corr plot to decide alpha, warning=FALSE}
set.seed(1234)
# Split the matrix
COR <- data.matrix(t_data[,-ncol(t_data)])

# Get the number of columns in the original matrix
num_columns <- ncol(COR)

# Randomly select 1000 column indices
selected_columns <- sample(1:num_columns, 40, replace = FALSE)
selected_columns2 <- sample(1:num_columns, 40, replace = FALSE)
selected_columns3 <- sample(1:num_columns, 40, replace = FALSE)

# Create a new matrix with only the selected columns
selected_matrix <- COR[, selected_columns]
selected_matrix2 <- COR[, selected_columns2]
selected_matrix3 <- COR[, selected_columns3]

# Set the shrinkage parameter (lambda)
#lambda <- 0.1
# Use shrinkage estimation
#Sigma_shrinkage <- cor.shrink(selected_matrix, lambda)
#Sigma_shrinkage <- data.matrix(Sigma_shrinkage)

# Use HiClimR and OpenBLAS to avoid approximations
xcor1 <- fastCor(selected_matrix, upperTri = FALSE, nSplit = 5, optBLAS = TRUE)
xcor2 <- fastCor(selected_matrix2, upperTri = FALSE, nSplit = 5, optBLAS = TRUE)
xcor3 <- fastCor(selected_matrix3, upperTri = FALSE, nSplit = 5, optBLAS = TRUE)

# Plot correlation matrices
corrplot(xcor1, title = "First sampling", order = "hclust", 
         tl.col = "black", tl.srt = 90)
corrplot(xcor2, title = "Second sampling", order = "hclust", 
         tl.col = "black", tl.srt = 90)
corrplot(xcor3, title = "Third sampling", order = "hclust", 
         tl.col = "black", tl.srt = 90)

# Remove unnecessary objects
rm(COR, num_columns, selected_columns, selected_columns2, selected_columns3,
   selected_matrix, selected_matrix2, selected_matrix3, xcor1, xcor2, xcor3)

```


```{r Tuning LASSO}
set.seed(1234)
# Number coating the values
## Specify the columns to be label encoded
columns_to_encode <- c("molecular.group.ch1")
Lasso_data <- t_data %>% mutate_at(columns_to_encode, as.factor)
Lasso_data_f <- Lasso_data
Lasso_data$molecular.group.ch1 <- as.numeric(Lasso_data$molecular.group.ch1)

# Standardize predictors (recommended for lasso)
X <- as.matrix(Lasso_data[, -which(colnames(Lasso_data) == "molecular.group.ch1")])

# Fit cross-validated Lasso model
cv_lasso <- cv.glmnet(x = X,
                      y = Lasso_data$molecular.group.ch1, alpha = 0.8, grouped = FALSE)

optimal_lambda <- cv_lasso$lambda.min

# Remove unnecessary objects
rm(columns_to_encode, cv_lasso)

```

```{r Fit LASSO}
set.seed(1234)
# Extract predictors and response
X <- as.matrix(subset(Lasso_data, select = -ncol(Lasso_data)))  # Exclude the response variable
y <- as.matrix(Lasso_data$molecular.group.ch1)

# Fit a lasso regression model
lasso_model <- glmnet(X, y, alpha = 0.8, lambda = optimal_lambda)

# Display selected features
plot(coef(lasso_model, s = optimal_lambda))

to_filter <- names(lasso_model$beta[, 1][lasso_model$beta[, 1] != 0])
names_imp <- data.frame(lasso_model$beta[, 1][lasso_model$beta[, 1] != 0])
# Extract non-zero coefficients from the Lasso model
selected_features <- coef(lasso_model, s = optimal_lambda, exact = TRUE, x = X, y = y)

# Filter the original dataset based on selected features
df_filtered <- subset(Lasso_data_f, select = to_filter)
new_colnames <- paste0("i", colnames(df_filtered))
colnames(df_filtered) <- new_colnames
df_filtered <- cbind(df_filtered, disease_state)

# Remove unnecessary objects
rm(X, y, to_filter, selected_features,new_colnames,optimal_lambda)

```

# Split for Classification

```{r Split}
# Number coating the values
## Specify the columns to be label encoded
columns_to_encode <- c("molecular.group.ch1")

# Subset the dataset for training where molecular.group.ch1 is not "Unclassified"
Train_data <- subset(df_filtered, molecular.group.ch1 != "Unclassified")

# Convert specified columns to factor type for the training dataset
Train_data <- Train_data %>% mutate_at(columns_to_encode, as.factor)

# Subset the dataset for unclassified entries
New_data <- subset(df_filtered, molecular.group.ch1 == "Unclassified")
New_data <- subset(New_data, select = -molecular.group.ch1)

# Create a dictionary-like structure to store the labels
## The order corresponds to the number
my_levels <- levels(Train_data$molecular.group.ch1)

# Set seed for reproducibility
set.seed(1234)

# Create an index for splitting the data
index <- createDataPartition(Train_data$molecular.group.ch1, p = 0.8, list = FALSE)

# Create the training set
train <- Train_data[index, ]

# Create the testing set
test <- Train_data[-index, ]

# Convert molecular.group.ch1 to numeric for the entire dataset
Train_data$molecular.group.ch1 <- as.numeric(Train_data$molecular.group.ch1)

# Create the training set with numeric molecular.group.ch1
train_n <- Train_data[index, ]

# Create the testing set with numeric molecular.group.ch1
test_n <- Train_data[-index, ]

# Remove unnecessary objects
rm(columns_to_encode,index)


```

# Random Forest

```{r Tune RF}
# Tune the Random Forest model
model_tuned <- tuneRF(
               x = train,                   # Define predictor variables
               y = train$molecular.group.ch1,  # Define response variable
               ntreeTry = 500,                      # Number of trees to try
               mtryStart = 2,                       # Starting value for mtry (number of variables randomly sampled as candidates at each split
               stepFactor = 1.5,                    # Factor by which mtry is multiplied in each iteration
               improve = 0.5,                      # Minimum improvement in node impurity for a split to occur
               trace = FALSE,                       # Don't show real-time progress
               plot = TRUE,                         # Plot error rates during tuning
               importance = FALSE,                   # Calculate variable importance
               splitrule = "gini"
               )
rm(model_tuned)
```

```{r RF with cross validation from caret}

# Fit the Random Forest model
set.seed(1232)

# Define the control parameters for cross-validation
ctrl <- trainControl(method = "cv",number = 10)  

# Train the Random Forest model using cross-validation
model_RF <- train(molecular.group.ch1 ~ .,   
                  data = train,
                  method = "ranger",   # Random Forest method
                  trControl = ctrl,
                  importance = "impurity",
                  num.trees = 500)

# Display fitted models
print(model_RF)   # Using print for a more informative display

# Make predictions on the test set
test_RF <- predict(model_RF, newdata = test)

# Make predictions on the new data
Predict_RF <- data.frame(predict(model_RF, newdata = New_data))
Predict_RF$disease_state <- Predict_RF$predict.model_RF..newdata...New_data.
Predict_RF$predict.model_RF..newdata...New_data. <- NULL
rownames(Predict_RF) <- rownames(New_data)

# Extract variable importance from the trained Random Forest model
var_imp <- varImp(model_RF)
var_imp <- var_imp[["importance"]]
top_20 <- var_imp %>% arrange(desc(Overall))
top_20 <- head(top_20,20)
colnames(top_20) <- c("Overall")

ggplot(top_20, aes(x = Overall, y = reorder(row.names(top_20), Overall))) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Top 20 Values for RF", x = "Importance", y = "Probe_ID") +
  theme_minimal()

# Remove unnecessary objects
rm(test_RF, model_RF,ctrl,var_imp,top_20)


```

```{r Deafult RF after Tune RF}
# Fit the Random Forest model
set.seed(1232)


model_RF <- randomForest(molecular.group.ch1 ~ ., 
                      data = train,
                      ntree = 500,
                      splitrule = "gini",
                      importance = TRUE,
                      mtry = 2
)

# Make predictions on the test set
test_RF <- predict(model_RF, newdata = test)

# Make predictions on the new data
Prediction_RF <- predict(model_RF, newdata = New_data)

# View predictions
Predict_RF_2 <- data.frame(Prediction_RF)
#predicted_RF$predicted_rf <- predict_RF$Prediction_RF

# Produce a variable importance plot
varImpPlot(model_RF, main = "Variable Importance Plot", col = "darkgreen")

# Evaluate the model's performance on the test set
cm <- confusionMatrix(test_RF, test$molecular.group.ch1)
cm

# Remove unnecessary objects
rm(test_RF,model_RF, Prediction_RF, cm)



```

# XGBoost (Extreme Gradient Boosting):

-Type: Ensemble Learning (boosting method) -Description: XGBoost is a powerful and efficient implementation of gradient boosting. It sequentially adds weak learners (usually decision trees) to the model, each correcting errors of the previous one. -Working: It optimizes a loss function and includes regularization terms to avoid overfitting. The final prediction is a weighted sum of the predictions from all the weak learners.

```{r XGBoost (Extreme Gradient Boosting), }
# Set seed for reproducibility
set.seed(1234)

# Step 1: Prepare the data
X_train <- as.matrix(train_n[, -which(names(train_n) == "molecular.group.ch1")])  # Exclude the response variable
y_train <- as.matrix(train_n$molecular.group.ch1) - 1   # Response variable

# Transform the matrix into an xgb.DMatrix
X_train <- xgb.DMatrix(data = X_train, label = y_train)

# Step 2: Train the XGBoost model
xgb_model <- xgboost(data = X_train, 
                     objective = "multi:softmax", 
                     num_class = 2,
                     max.depth = 10,          # Maximum depth of each tree in the boosting process
                     nthread = 15,            # Number of threads to use during training
                     nrounds = 500,            # Number of boosting rounds (number of trees to build)
                     verbose = 0)

# Step 3: Make predictions on the new data
X_predict <- test_n[, -ncol(test_n)]  # Exclude the response variable

# Predict the class labels 
predictions_xgb <- data.frame(predict(xgb_model, as.matrix(X_predict)) + 1)
predictions_xgb$predicted_xgb <- predictions_xgb$predict.xgb_model..as.matrix.X_predict.....1
predictions_xgb$predict.xgb_model..as.matrix.X_predict.....1 <- NULL

# Predict the class labels for New_data
xgb_predictions <- data.frame(predict(xgb_model, as.matrix(New_data)) + 1)
xgb_predictions$disease_state <- xgb_predictions$predict.xgb_model..as.matrix.New_data.....1
xgb_predictions$predict.xgb_model..as.matrix.New_data.....1 <- NULL
rownames(xgb_predictions) <- rownames(New_data)

Predict_xgb <- xgb_predictions %>%
  mutate(disease_state = case_when(
    disease_state == 1 ~ my_levels[1],
    disease_state == 2 ~ my_levels[2]
  ))

# Create a confusion matrix
conf_matrix <- table(predictions_xgb$predicted_xgb, test_n$molecular.group.ch1)
conf_matrix

# Calculate metrics
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- diag(conf_matrix) / rowSums(conf_matrix)
recall <- diag(conf_matrix) / colSums(conf_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Create a bar plot for precision, recall, and F1 score
metrics_df_xgb <- tibble(Metric = c("Accuracy","Precision", "Recall", "F1 Score"),
                         Value = c(mean(accuracy, na.rm = TRUE),
                                   mean(precision, na.rm = TRUE),
                                   mean(recall, na.rm = TRUE),
                                   mean(f1_score, na.rm = TRUE)))

metrics_df_xgb
#print(xgb_model)
summary(xgb_model)

# Remove unnecessary objects
rm(X_train, y_train, X_predict, predictions_xgb, xgb_predictions, conf_matrix, accuracy, precision, recall, f1_score, metrics_df_xgb)

```

# SVM (Support Vector Machines)

```{r Do SVM}
set.seed(1234)
# Define the training control with cross-validation
ctrl <- trainControl(method = "cv", number = 10, classProbs = T)

# Specify the tuning parameter grid for the SVM
tune_grid <- expand.grid(cost = c(0.001,0.01), Loss = 0:7)

# Train the SVM model using caret
svm_model <- train(molecular.group.ch1 ~ ., 
                   data = train, 
                   method = "svmLinear3", 
                   trControl = ctrl, 
                   tuneGrid = tune_grid)
# Print the best parameters
print(svm_model)

# Make predictions on the test set
predictions <- predict(svm_model, newdata = test[,-ncol(test)])

# Evaluate the model performance
confusionMatrix(data = predictions, reference = test$molecular.group.ch1)

# Predict the class labels for New_data
Predict_svm <- data.frame(predict(svm_model, newdata = New_data))
Predict_svm$disease_state <- Predict_svm$predict.svm_model..newdata...New_data.
Predict_svm$predict.svm_model..newdata...New_data. <- NULL
rownames(Predict_svm) <- rownames(New_data)

# Extract variable importance from the trained Random Forest model
var_imp <- varImp(svm_model)
plot(var_imp, main = "Importance for SVM")
var_imp <- var_imp[["importance"]]
top_20 <- subset(var_imp, select = Fibrosis)
colnames(top_20) <- c("Overall")
top_20 <- top_20 %>% arrange(desc(Overall))
top_20 <- head(top_20,20)

ggplot(top_20, aes(x = Overall, y = reorder(row.names(top_20), Overall))) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Top 20 Values for SVM", x = "Importance", y = "Probe_ID") +
  theme_minimal()


# Remove unnecessary objects
rm(ctrl, tune_grid,var_imp,svm_model,top_20)

```

# K-Nearest Neighbors (KNN)

```{r}
set.seed(1234)
# Define the training control with cross-validation
ctrl <- trainControl(method = "cv", number = 10)

#tuneGrid <- expand.grid(k = 1:10 ,)
tuneGrid <- expand.grid(kmax = 2:3, distance = 2:3, kernel = c("optimal", "rectangular","gaussian"))

knn_model <- train(molecular.group.ch1 ~ ., data = train, method = "kknn",
                     trControl = ctrl, tuneGrid = tuneGrid)

# Print the best parameters
print(knn_model)

# Make predictions on the test set
predictions <- predict(knn_model, newdata = test[,-ncol(test)])

# Evaluate the model performance
confusionMatrix(data = predictions, reference = test$molecular.group.ch1)
#conf_matrix <- table(predictions, test$molecular.group.ch1)
#print(conf_matrix)

Predict_knn <- data.frame(predict(knn_model, newdata = New_data))
Predict_knn$disease_state <- Predict_knn$predict.knn_model..newdata...New_data.
Predict_knn$predict.knn_model..newdata...New_data. <- NULL
rownames(Predict_knn) <- rownames(New_data)

# Extract variable importance from the trained Random Forest model
var_imp <- varImp(knn_model)
plot(var_imp, main = "Importance for KNN")
var_imp <- var_imp[["importance"]]
top_20 <- subset(var_imp, select = Fibrosis)
colnames(top_20) <- c("Overall")
top_20 <- top_20 %>% arrange(desc(Overall))
top_20 <- head(top_20,20)

ggplot(top_20, aes(x = Overall, y = reorder(row.names(top_20), Overall))) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Top 20 Values for SVM", x = "Importance", y = "Probe_ID") +
  theme_minimal()

rm(ctrl, tuneGrid, predictions)

```

