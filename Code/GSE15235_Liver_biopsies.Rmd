---
title: "Exam_Lauria"
author: "Thomas Sirchi"
date: "`r Sys.Date()`"
output: html_document
---

## Loading packages

Store package names in a vectors for ease of access and to load them easily

```{r setup, message=FALSE, warning=FALSE}
# List of R packages used in the script with a brief description of their purposes
PACKAGES <- c(
  "GEOquery",        # For accessing and retrieving Gene Expression Omnibus (GEO) data
  "limma",           # For linear modeling of microarray data
  "ggplot2",         # To create and customize plots, and save images
  "plotly",          # Interactive plotting library
  "ggcorrplot",      # For correlation matrix visualization with ggplot2
  "RColorBrewer",    # Color palettes for better visualizations
  "tidyverse",       # A collection of packages for easy and tidy data handling
  "dplyr",           # Efficient data manipulation and transformation
  "factoextra",      # Provides additional functions for enhanced ggplot-based plots
  "FactoMineR",      # For multivariate exploratory data analysis
  "cluster",         # To perform PAM (Partitioning Around Medoids)
  "HiClimR",         # For hierarchical clustering analysis
  "glmnet",          # To perform LASSO regression for features selection 
  "caret",           # For machine learning model training and evaluation
  "RRF",          # For random forest modeling and feature importance
  "kknn",            # For k-nearest neighbors classification
  "LiblineaR",       # For linear classification and regression using LIBLINEAR library
  "biomaRt",         # Interface to access BioMart databases
  "enrichR",         # For gene set enrichment analysis
  "corpcor",         # For robust correlation matrix estimation
  "e1071",           # For various statistical procedures
  "rScudo",          # To perform SCUDO (Signature-based Clustering for Diagnostic Purposes)
  "corrplot",         # For visually representing correlation matrices
  "EnhancedVolcano"
)

invisible(lapply(PACKAGES, library, character.only = TRUE))
gc()
# Install packages (uncomment the line below to install)
# install.packages(PACKAGES)

```


Print current system info, R and packages versions (for reproducibility)

```{r sessionInfo}
sessionInfo()
```

## Data retrieval

Retrieve dataset from GEO

```{r Retrieve dataset from GEO, message=FALSE, warning=FALSE}
gset <- getGEO("GSE15235", GSEMatrix =TRUE)
```

Extract the data from the dataset

```{r Extract the data, message=FALSE, warning=FALSE}
setwd("D:/VarieTHOM/University/QCB/4_SEMESTRE/Advanced Data Analysis")
exp_data <- data.frame(gset[["GSE15235_series_matrix.txt.gz"]]@assayData[["exprs"]])
exp_data <- na.omit(exp_data)

# Log trasform the data
mydata_log <- data.frame(log(exp_data))
#write.csv(mydata_log, file = 'GSE15235_log.csv', row.names = TRUE)

```

# Metatdata manipulation

recover the gene ids and disease state from the metadata.

```{r Metadata extraction}
setwd("D:/VarieTHOM/University/QCB/4_SEMESTRE/Advanced Data Analysis")
# general metatdata on the disesase
metadata <- data.frame(gset[["GSE15235_series_matrix.txt.gz"]]@phenoData@data)
#write.csv(metadata, file = 'metadata1.csv', row.names = TRUE)

disease_state <- subset(metadata, select = molecular.group.ch1)

# general metatdata on the Affimatrix, experiment, gene symbol etc. 
metadata2 <- data.frame(gset[["GSE15235_series_matrix.txt.gz"]]@featureData@data)
#write.csv(metadata2, file = 'metadata2.csv', row.names = TRUE)

# Extract Gene Symbols from a column in the 'metadata2' data frame
Gene_Symbols <- data.frame(Gene.Symbol = gsub("[ -/].*", "", metadata2$Gene.Symbol),
                           row.names = rownames(exp_data))
Gene_Symbols$probe_id <- rownames(Gene_Symbols)

# Replace empty strings in the "Gene.Symbol" column with NA
Gene_Symbols <- Gene_Symbols %>%
  mutate(Gene.Symbol = na_if(Gene.Symbol, ""))
  

# Remove
rm(metadata2)

```

## Fist view of the data

```{r Boxplots, message=FALSE, warning=FALSE}

# Create boxplot for original data
boxplot(head(exp_data,2000), main = "Original Data", col = "skyblue")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at 0

# Create boxplot for log data
boxplot(head(mydata_log,2000), main = "Log Data", col = "pink")
abline(h = 0, lty = 2, col = "red")

# Data Preprocessing: center the log data
preProcValues <- preProcess(mydata_log, method = c("center"))
mydata_log_centered <- predict(preProcValues, mydata_log)
mydata_log <- mydata_log_centered
# Transpose and add a response variable for future
t_data <- t(mydata_log)
t_data <- cbind(t_data, disease_state)
# Create boxplot for centered data, is it useful?
boxplot(head(mydata_log,2000), main = "Log Data centered", col = "pink")
abline(h = 0, lty = 2, col = "red")

rm(preProcValues,mydata_log_centered)

```

## Unsupervised methods (machine learning methods)
#PCA

```{r Principal component analysis PCA, message=FALSE, warning=FALSE}

# Perform Principal Component Analysis (PCA) on the transpose of the cleaned data
res_pca <- prcomp(t(mydata_log))

# Extract eigenvalues and variance contributions from the PCA results
eig_PCA <- get_eig(res_pca)

# Transform the principal components into a data frame
components <- res_pca[["x"]]
components <- data.frame(components)

# Combine the principal components with the original disease_state metadata
components <- cbind(components, disease_state)

# Trigger garbage collection to free up memory
gc()

# Remove unnecessary objects
rm(eig_PCA)

```

```{r Scree plot, message=FALSE, warning=FALSE}
# Visualize eigenvalues from PCA
fviz_eig(res_pca,addlabels = TRUE)
rm(res_pca)
```

```{r Plot PCA, message=FALSE, warning=FALSE}

# Create a 2D scatter plot using Plotly
fig2D <- plot_ly(components, 
                 x = ~PC1, y = ~PC2, 
                 color = components$molecular.group.ch1,
                 colors = brewer.pal(n = 4, name = "RdBu"),  
                 symbol = components$molecular.group.ch1, 
                 symbols = c('diamond', 'circle', 'cross'), 
                 mode = 'markers',
                 type = "scatter")

# Display the 2D scatter plot
fig2D

# Remove the 2D scatter plot object to free up memory
rm(fig2D)

```

# Uniform Manifold Approximation and Projection

```{r}

#library(umap)
umap_try <- umap(t_data[,-ncol(t_data)],n_neighbors = 15,method = "naive")

umap_try <- as.data.frame(umap_try$layout)
rownames(umap_try) <- rownames(t_data)
# Create a 2D scatter plot using Plotly
fig2D <- plot_ly(umap_try, 
                 x = ~V1, y = ~V2, 
                 color = cluster_assignments$cluster_assignments,
                 colors = brewer.pal(n = 4, name = "RdBu"),  
                 symbol = t_data$molecular.group.ch1, 
                 symbols = c('diamond', 'circle', 'cross'), 
                 mode = 'markers',
                 type = "scatter")
#t_data$molecular.group.ch1
# Display the 2D scatter plot
fig2D

# Remove the 2D scatter plot object to free up memory
rm(fig2D)


```

# PAM (Partitioning Around Medoids)

```{r Find best K , message=FALSE, warning=FALSE}
set.seed(1234)

# Visualize the optimal number of clusters using the elbow method
fviz_nbclust(t(mydata_log), FUNcluster = cluster::pam, k.max = 5)

# Visualize the optimal number of clusters using the within-cluster sum of squares method
fviz_nbclust(t(mydata_log), FUNcluster = cluster::pam, k.max = 5, method = "wss")

```

```{r Compute PAM , message=FALSE, warning=FALSE}
set.seed(1234)
K <- 3

# Perform PAM clustering
pam_result <- pam(t(mydata_log), K)

# Access cluster assignments and medoids
cluster_assignments <- pam_result$clustering
medoids <- pam_result$medoids

# Display a table of cluster assignments and disease states
table(cluster_assignments, t(disease_state))

# Plot the results (for 2D data)
plot(t(mydata_log), col = cluster_assignments, pch = 16, main = "PAM Clustering")
points(medoids, col = 1:K, pch = 3, cex = 2)

# Store cluster assignments in components$pam
#components <- cbind(components, cluster_assignments)
cluster_assignments <- data.frame(cluster_assignments)
rownames(cluster_assignments) <- rownames(t_data)
# Remove unnecessary objects
rm(pam_result,K,medoids)


```

# hierarchical clustering

```{r hierarchical, message=FALSE, warning=FALSE}
# Calculate distances between observations and create a simple dendrogram
dm <- dist(t(mydata_log))
hc <- hclust(dm, method = 'average')
plot(hc, hang = -1)
rect.hclust(hc, k = 3, border = 'red')

# Assign cluster memberships
clust.vec.2 <- cutree(hc, k = 3)

# Visualize clusters
fviz_cluster(list(data = t(mydata_log), cluster = clust.vec.2))

# Remove unnecessary objects
rm(dm, hc, clust.vec.2)

```

# Supervised learning techniques

```{r Create parallel cluster}
cl <- parallel::makePSOCKcluster(3)
doParallel::registerDoParallel(cl)
```


## LASSO (Least Absolute Shrinkage and Selection Operator)

```{r Corr plot to decide alpha, warning=FALSE}
set.seed(1234)
# Split the matrix
COR <- data.matrix(t_data[,-ncol(t_data)])

# Get the number of columns in the original matrix
num_columns <- ncol(COR)

# Randomly select 1000 column indices
selected_columns <- sample(1:num_columns, 40, replace = FALSE)
selected_columns2 <- sample(1:num_columns, 40, replace = FALSE)
selected_columns3 <- sample(1:num_columns, 40, replace = FALSE)

# Create a new matrix with only the selected columns
selected_matrix <- COR[, selected_columns]
selected_matrix2 <- COR[, selected_columns2]
selected_matrix3 <- COR[, selected_columns3]

# Set the shrinkage parameter (lambda)
#lambda <- 0.1
# Use shrinkage estimation
#Sigma_shrinkage <- cor.shrink(selected_matrix, lambda)
#Sigma_shrinkage <- data.matrix(Sigma_shrinkage)

# Use HiClimR and OpenBLAS to avoid approximations
xcor1 <- fastCor(selected_matrix, upperTri = FALSE, nSplit = 5, optBLAS = TRUE)
xcor2 <- fastCor(selected_matrix2, upperTri = FALSE, nSplit = 5, optBLAS = TRUE)
xcor3 <- fastCor(selected_matrix3, upperTri = FALSE, nSplit = 5, optBLAS = TRUE)

# Plot correlation matrices
corrplot(xcor1, title = "First sampling", order = "hclust", 
         tl.col = "black", tl.srt = 90)
corrplot(xcor2, title = "Second sampling", order = "hclust", 
         tl.col = "black", tl.srt = 90)
corrplot(xcor3, title = "Third sampling", order = "hclust", 
         tl.col = "black", tl.srt = 90)

# Remove unnecessary objects
rm(COR, num_columns, selected_columns, selected_columns2, selected_columns3,
   selected_matrix, selected_matrix2, selected_matrix3, xcor1, xcor2, xcor3)

```


```{r Tuning LASSO}
set.seed(1234)
# Number coating the values
## Specify the columns to be label encoded
columns_to_encode <- c("molecular.group.ch1")
Lasso_data <- t_data %>% 
  mutate_at(columns_to_encode, as.factor)
Lasso_data_f <- Lasso_data
Lasso_data$molecular.group.ch1 <- as.numeric(Lasso_data$molecular.group.ch1)

# Standardize predictors (recommended for lasso)
X <- as.matrix(Lasso_data[, -which(colnames(Lasso_data) == "molecular.group.ch1")])

# Fit cross-validated Lasso model
cv_lasso <- cv.glmnet(x = X,
                      y = Lasso_data$molecular.group.ch1,
                      family = "gaussian",
                      alpha = 0.5, 
                      grouped = FALSE,
                      parallel = TRUE,
                      relax = TRUE,
                      type.measure = "mse",
                      type.gaussian = "naive",
                      nfolds = 10)
plot(cv_lasso)
optimal_lambda <- cv_lasso$lambda.min

# Remove unnecessary objects
rm(columns_to_encode, cv_lasso)

```

```{r Fit LASSO}
set.seed(1234)
# Extract predictors and response
X <- as.matrix(subset(Lasso_data, select = -ncol(Lasso_data)))  # Exclude the response variable
y <- as.matrix(Lasso_data$molecular.group.ch1)

# Fit a lasso regression model
lasso_model <- glmnet(X, y, 
                      alpha = 0.5, 
                      lambda = optimal_lambda,
                      family = "gaussian",
                      parallel = TRUE,
                      type.measure = "mse",
                      type.gaussian = "naive",
                      relax = TRUE)

# Display selected features
plot(coef(lasso_model, s = optimal_lambda))

to_filter <- names(lasso_model$beta[, 1][lasso_model$beta[, 1] != 0])
names_imp <- data.frame(lasso_model$beta[, 1][lasso_model$beta[, 1] != 0])
# Extract non-zero coefficients from the Lasso model
selected_features <- coef(lasso_model, s = optimal_lambda, exact = TRUE, x = X, y = y)

# Filter the original dataset based on selected features
df_filtered <- subset(Lasso_data_f, select = to_filter)
new_colnames <- paste0("i", colnames(df_filtered))
colnames(df_filtered) <- new_colnames
df_filtered <- cbind(df_filtered, disease_state)

# Remove unnecessary objects
rm(X, y, to_filter, selected_features,new_colnames,optimal_lambda,names_imp,Lasso_data_f,Lasso_data, lasso_model)

```

# Split for Classification

```{r Split}
# Set seed for reproducibility
set.seed(1234)
setwd("D:/VarieTHOM/University/QCB/4_SEMESTRE/Advanced Data Analysis")
#write.csv(df_filtered, file = 'Probes_post_LASSO.csv', row.names = TRUE)
# Number coating the values
## Specify the columns to be label encoded
columns_to_encode <- c("molecular.group.ch1")

# Subset the dataset for training where molecular.group.ch1 is not "Unclassified"
Train_data <- subset(df_filtered, molecular.group.ch1 != "Unclassified")

# Convert specified columns to factor type for the training dataset
Train_data <- Train_data %>% mutate_at(columns_to_encode, as.factor)

# Subset the dataset for unclassified entries
New_data <- subset(df_filtered, molecular.group.ch1 == "Unclassified")
New_data <- subset(New_data, select = -molecular.group.ch1)

# Create a dictionary-like structure to store the labels
## The order corresponds to the number
my_levels <- levels(Train_data$molecular.group.ch1)

# Create an index for splitting the data
index <- createDataPartition(Train_data$molecular.group.ch1, p = 0.7, list = FALSE)

# Create the training set
train <- Train_data[index, ]

# Create the testing set
test <- Train_data[-index, ]

# Convert molecular.group.ch1 to numeric for the entire dataset
Train_data$molecular.group.ch1 <- as.numeric(Train_data$molecular.group.ch1)

# Create the training set with numeric molecular.group.ch1
train_n <- Train_data[index, ]

# Create the testing set with numeric molecular.group.ch1
test_n <- Train_data[-index, ]

# Remove unnecessary objects
rm(columns_to_encode,index)


```

# Random Forest

```{r RF with cross validation from caret}
set.seed(1232)

# Define the control parameters for cross-validation
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10,
                     repeats = 3,
                     adaptive = list(min = 5, alpha = 0.05,method = "gls", complete = TRUE),
                     classProbs = TRUE,
                     search = "random",
                     sampling = "down")

tuneGrid <- expand.grid(mtry = 2:5, coefReg = c(0.75,0.85,0.95))

# Train the Random Forest model using cross-validation
model_RF <- train(molecular.group.ch1 ~ .,   
                  data = train,
                  method = "RRFglobal",
                  num.trees = 500,
                  importance = TRUE,
                  trControl = ctrl,
                  tuneGrid = tuneGrid)

# Display fitted models
print(model_RF)   # Using print for a more informative display
plot(model_RF)

# Make predictions on the test set
predictions <- predict(model_RF, newdata = test[,-ncol(test)])

# Evaluate the model performance
cm = confusionMatrix(data = predictions, reference = test$molecular.group.ch1, mode = "everything", positive = NULL)
cm

# Make predictions on the test set
test_RF <- predict(model_RF, newdata = test)

# Make predictions on the new data
Predict_RF <- data.frame(predict(model_RF, newdata = New_data))
Predict_RF$disease_state <- Predict_RF$predict.model_RF..newdata...New_data.
Predict_RF$predict.model_RF..newdata...New_data. <- NULL
rownames(Predict_RF) <- rownames(New_data)

# Extract variable importance from the trained Random Forest model
var_imp <- varImp(model_RF)
plot(var_imp, main = "Importance for RF",top = 20)

# Remove unnecessary objects
rm(test_RF, model_RF,ctrl,var_imp)


```

# SVM (Support Vector Machines)

```{r Do SVM}
set.seed(1234)
# Define the training control with cross-validation
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10,
                     repeats = 3, 
                     adaptive = list(min = 5, alpha = 0.05,method = "gls", complete = TRUE),
                     search = "random",
                     sampling = "down")

# Specify the tuning parameter grid for the SVM
tune_grid <- expand.grid(cost = c(0.01,0.015,0.02), Loss = 1:7)

# Train the SVM model using caret
svm_model <- train(molecular.group.ch1 ~ ., 
                   data = train, 
                   method = "svmLinear3", 
                   trControl = ctrl,
                   importance = TRUE,
                   tuneGrid = tune_grid)
# Print the best parameters
print(svm_model)
plot(svm_model)

# Make predictions on the test set
predictions <- predict(svm_model, newdata = test[,-ncol(test)])

# Evaluate the model performance
cm = confusionMatrix(data = predictions, reference = test$molecular.group.ch1, mode = "everything")
cm

# Predict the class labels for New_data
Predict_SVM <- data.frame(predict(svm_model, newdata = New_data))
Predict_SVM$disease_state <- Predict_SVM$predict.svm_model..newdata...New_data.
Predict_SVM$predict.svm_model..newdata...New_data. <- NULL
rownames(Predict_SVM) <- rownames(New_data)

# Extract variable importance from the trained Random Forest model
var_imp <- varImp(svm_model)
plot(var_imp, main = "Importance for SVM",top = 20)

# Remove unnecessary objects
rm(ctrl, tune_grid,var_imp,svm_model)

```

# K-Nearest Neighbors (KNN)

```{r Apply KKNN from caret}
set.seed(1234)
# Define the training control with cross-validation
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10,
                     repeats = 3,
                     adaptive = list(min = 5, alpha = 0.05,method = "gls", complete = TRUE),
                     classProbs = TRUE,
                     search = "random",
                     sampling = "down")


tuneGrid <- expand.grid(kmax = 2:5, 
                        distance = 1:3, 
                        kernel = c("optimal", "rectangular"))

knn_model <- train(molecular.group.ch1 ~ .,
                   data = train, 
                   method = "kknn",
                   trControl = ctrl,
                   importance = TRUE,
                   tuneGrid = tuneGrid)

# Print the best parameters
print(knn_model)
plot(knn_model)

# Make predictions on the test set
predictions <- predict(knn_model, newdata = test[,-ncol(test)])

# Evaluate the model performance
confusionMatrix(data = predictions, reference = test$molecular.group.ch1)

Predict_knn <- data.frame(predict(knn_model, newdata = New_data))
Predict_knn$disease_state <- Predict_knn$predict.knn_model..newdata...New_data.
Predict_knn$predict.knn_model..newdata...New_data. <- NULL
rownames(Predict_knn) <- rownames(New_data)

# Extract variable importance from the trained Random Forest model
var_imp <- varImp(knn_model)
plot(var_imp, main = "Importance for KNN",top = 20)

rm(ctrl, tuneGrid, predictions, knn_model)


```


# Stop cluster
```{r Stop parallel}
parallel::stopCluster(cl)
rm(cl)
```

# DEGS with limma
```{r Run the degs}
set.seed(1234)

bt <- metadata$molecular.group.ch1

# Replace 'a' values with 'b'
disease_state$molecular.group.ch1 <- gsub('Unclassified', 'Inflammation', bt)
# Create a design matrix without an intercept for two groups
design <- model.matrix(~ 0 + disease_state$molecular.group.ch1)
colnames(design) <- c("Fibrosis","Inflammation")

# Fit a linear model to the log-transformed data
fit <- lmFit(mydata_log, design)

# Specify contrasts of interest between Fibrosis and Inflammation
contrast.matrix <- makeContrasts(Fibrosis - Inflammation, levels = design)

# Apply empirical Bayes moderation to the linear model fit
fit <- eBayes(fit)

# Extract the top differentially expressed genes based on the specified contrast
de.genes <- topTable(fit, coef = 1,p.value = 0.01, sort.by = "logFC", number=Inf,adjust="BH")
de.genes2 <- topTable(fit, coef = 2,p.value = 0.01, sort.by = "logFC", number=Inf,adjust="BH")

# Create a volcano plot to visualize the differential expression results
#volcanoplot(fit, coef=1, names=fit$genes$ID, xlab="Log Fold Change", ylab="Log Odds", highlight = 10)

# Assuming de.genes is your data frame with the differential expression results
volcano_plot1 <- EnhancedVolcano(de.genes,
                                lab = rownames(de.genes),
                                x = 'logFC',
                                y = 'P.Value',
                                title = 'Volcano plot of DE genes',
                                pCutoff = 0.0001,
                                FCcutoff = 0.8,
                                pointSize = 2.0,
                                labSize = 4.0,
                                legendLabSize = 10,
                                legendIconSize = 3.0,
                                drawConnectors = TRUE,
                                widthConnectors = 0.5)

print(volcano_plot1)


# Assuming de.genes is your data frame with the differential expression results
volcano_plot2 <- EnhancedVolcano(de.genes2,
                                lab = rownames(de.genes2),
                                x = 'logFC',
                                y = 'P.Value',
                                title = 'Volcano plot of DE genes',
                                pCutoff = 0.00001,
                                FCcutoff = 0.8,
                                pointSize = 2.0,
                                labSize = 4.0,
                                legendLabSize = 10,
                                legendIconSize = 3.0,
                                drawConnectors = TRUE,
                                widthConnectors = 0.5)

print(volcano_plot2)

rm(volcano_plot1,volcano_plot2,fit,bt,design,contrast.matrix)
```

```{r DEGs results}

filtered_Fibrosis <- de.genes %>%
  dplyr::select(logFC, P.Value) %>%
  mutate(probe_id = rownames(de.genes)) %>%
  filter(logFC > 0.8 | logFC < -0.8 & P.Value < 0.0001)  %>%
  arrange(logFC)

filtered_Inflammation <- de.genes2 %>%
  dplyr::select(logFC, P.Value) %>%
  mutate(probe_id = rownames(de.genes2)) %>%
  filter(logFC > 0.8 | logFC < -0.8 & P.Value < 0.00001)  %>%
  arrange(logFC)

# Found gene name
filtered_Fibrosis <- filtered_Fibrosis %>% 
  inner_join(Gene_Symbols, by = "probe_id")

filtered_Inflammation <- filtered_Inflammation %>% 
  inner_join(Gene_Symbols, by = "probe_id")

rm(de.genes,de.genes2)

```

```{r}

# List available databases from Enrichr
dbs <- listEnrichrDbs()
dbs <- dbs[order(dbs$libraryName),]
Databases <- data.frame(dbs$libraryName)

# Enrichment analysis for DrugMatrix and IDG_Drug_Targets_2022 databases

# Define the databases for enrichment analysis
dbs_dd <- c("DrugMatrix")

dbs_bp<- c("GO_Biological_Process_2023","GO_Molecular_Function_2023","KEGG_2021_Human","WikiPathway_2023_Human")

# Perform enrichment analysis GO
Fibrosis_GO <- enrichr(genes = filtered_Fibrosis$Gene.Symbol, databases = dbs_bp)

Inflammation_GO <- enrichr(genes = filtered_Inflammation$Gene.Symbol, databases = dbs_bp)

plotEnrich(Fibrosis_GO[[1]], showTerms = 20, numChar = 60, y = "Count", orderBy = "Combined.Score")
plotEnrich(Inflammation_GO[[1]], showTerms = 20, numChar = 60, y = "Count", orderBy = "Combined.Score")

# Perform enrichment analysis drugmatrix
upClinical_Fibrosis <- enrichr(genes = filtered_Fibrosis$Gene.Symbol, databases = dbs_dd)

upClinical_Inflammation <- enrichr(genes = filtered_Inflammation$Gene.Symbol, databases = dbs_dd)

plotEnrich(upClinical_Fibrosis[[1]], showTerms = 20, numChar = 50, y = "Count", orderBy = "Combined.Score")

plotEnrich(upClinical_Inflammation[[1]], showTerms = 20, numChar = 50, y = "Count", orderBy = "Combined.Score")

rm(dbs,Databases)
```

# SCUDO draft
```{r test SCUDO}
set.seed(1234)
bt <- metadata$molecular.group.ch1

# Replace 'a' values with 'b'
bt <- gsub('Unclassified', 'Inflammation', bt)

bt <- as.factor(stringr::str_extract(bt, "^."))

# Extract the right object 
exp_set <- gset[["GSE15235_series_matrix.txt.gz"]]

inTrain <- caret::createDataPartition(bt, list = FALSE)
trainData <- exp_set[, inTrain]
testData <- exp_set[, -inTrain]

trainRes <- scudoTrain(trainData, groups = bt[inTrain], nTop = 100,
    nBottom = 100, alpha = 0.01)
trainRes

upSignatures(trainRes)[1:5,1:5]

consensusUpSignatures(trainRes)[1:5, ]

testNet <- scudoNetwork(trainRes, N = 0.25)
scudoPlot(testNet, vertex.label = NA)

```

```{r}
set.seed(1234)
testRes <- scudoTest(trainRes, testData, bt[-inTrain], nTop = 100,
    nBottom = 100)
testRes

testNet <- scudoNetwork(testRes, N = 0.25)
scudoPlot(testNet, vertex.label = NA)

testClust <- igraph::cluster_spinglass(testNet, spins = 2)
plot(testClust, testNet, vertex.label = NA)
```

```{r}
set.seed(1234)
classRes <- scudoClassify(trainData, testData, N = 0.25, nTop = 100,
    nBottom = 100, trainGroups = bt[inTrain], alpha = 0.1)

classRes$predicted

caret::confusionMatrix(classRes$predicted, bt[-inTrain])

rm(classRes)
```
